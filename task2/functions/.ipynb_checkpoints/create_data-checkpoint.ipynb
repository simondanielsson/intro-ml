{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc57707-37a5-4d4a-b169-017f97be133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9dc15-4d79-498c-ae28-9b50e9586067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "feature_path = \"../data/train_features.csv\"\n",
    "test_path = \"../data/test_features.csv\"\n",
    "label_path = \"../data/train_labels.csv\"\n",
    "\n",
    "X = pd.read_csv(feature_path)\n",
    "X_test = pd.read_csv(test_path)\n",
    "y = pd.read_csv(label_path)\n",
    "\n",
    "# Covert to time series df\n",
    "def to_timeseries(df: pd.DataFrame, colnames: List[str], time_steps: int, unique_id: str):\n",
    "    \"\"\"Coverts dataframe into time series version with unique identifier\"\"\"\n",
    "    X_new = pd.DataFrame()\n",
    "    \n",
    "    unique_ids = df.loc[:, unique_id].unique()\n",
    "    print(\"unique_ids:\", len(unique_ids))\n",
    "\n",
    "    #for id_ in unique_ids:\n",
    "    patient_rows = df[df[unique_id] == unique_ids[0]]\n",
    "    \n",
    "    #for index, row in patient_rows.iterrows():\n",
    "    #    X_new[index, \"\"]\n",
    "     \n",
    "to_timeseries(X, X.columns, 12, \"pid\")\n",
    "#### After ts\n",
    "\n",
    "ts_path = \"../data/data_p.csv\"\n",
    "X_ts = pd.read_csv(ts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4ef2a-3030-4b3e-8319-6a5751c3e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/train_features.csv\")\n",
    "d = data.shape[1]\n",
    "\n",
    "data_1hours = []\n",
    "data_p = pd.DataFrame()\n",
    "for i in range(12):\n",
    "    id_ith = range(i,len(data),12)\n",
    "    data_1hour = data.iloc[id_ith].iloc[:, [0] + [_ for _ in range(3, d)]]\n",
    "    \n",
    "    columns = data_1hour.columns\n",
    "    columns_new = [\"pid\"]\n",
    "    \n",
    "    # Construct new columns\n",
    "    for j in range(1,len(columns)):\n",
    "        col_new = columns[j] + \"-\" + str(i)\n",
    "        columns_new.append(col_new)\n",
    "    data_1hour.columns = columns_new\n",
    "    \n",
    "    data_1hours.append(data_1hour)\n",
    "\n",
    "    if data_p.empty:\n",
    "        data_p = data_1hour\n",
    "    else:\n",
    "        data_p = pd.merge(data_p,data_1hour,how=\"inner\",on=\"pid\") \n",
    "data_p\n",
    "data_p.to_csv(\"../data/train_features_transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39de844-6ce3-42e0-abd7-a859e74c3f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data(row):\n",
    "    return np.concatenate((\n",
    "        np.sum(np.isnan(row) + 0, axis=0), \n",
    "        12 - np.sum(np.isnan(row) + 0, axis=0),\n",
    "        np.nanmean(row, axis=0),\n",
    "        np.nanmax(row, axis=0),\n",
    "        np.nanmin(row, axis=0),\n",
    "        np.nanmedian(row, axis=0),\n",
    "        np.nanstd(row, axis=0),\n",
    "        np.nanquantile(row, 0.9, axis=0)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e4bb81-ff7b-4089-ae25-7e452a0720a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"/Users/simondanielsson/Documents/ETH/intro-to-ml/data_joschi/full_feature_set.csv\", header=None)\n",
    "path_features = \"../data/test_features.csv\"\n",
    "path_labels = \"../data/train_labels.csv\"\n",
    "\n",
    "raw_data = pd.read_csv(path_features)\n",
    "training_labels = pd.read_csv(path_labels)\n",
    "\n",
    "# Initialize New DataFrame By Columns\n",
    "\n",
    "raw_columns = list(raw_data.columns)\n",
    "used_columns = raw_columns\n",
    "not_used_columns = ['pid', 'Age', 'Time']\n",
    "\n",
    "for name in not_used_columns:\n",
    "    used_columns.remove(name)\n",
    "\n",
    "columns_max = used_columns.copy()\n",
    "for i, name in enumerate(columns_max):\n",
    "    columns_max[i] = columns_max[i] + '_max'\n",
    "\n",
    "columns_min = used_columns.copy()\n",
    "for i, name in enumerate(columns_min):\n",
    "    columns_min[i] = name + '_min'\n",
    "\n",
    "columns_med = used_columns.copy()\n",
    "for i, name in enumerate(columns_med):\n",
    "    columns_med[i] = name + '_med'\n",
    "\n",
    "features = ['pid', 'age'] + columns_min + columns_max + columns_med\n",
    "\n",
    "feature_set = pd.DataFrame(columns=features)\n",
    "\n",
    "ini_df = np.zeros((raw_data['pid'].nunique(), feature_set.shape[1]))\n",
    "feature_set = feature_set.append(pd.DataFrame(ini_df, columns=feature_set.columns), ignore_index=True)\n",
    "\n",
    "# Fill DataFrame by Grouped Patient Data\n",
    "\n",
    "grouped_features = raw_data.groupby(\"pid\")\n",
    "\n",
    "index_var = 0\n",
    "for name, data in grouped_features:\n",
    "\n",
    "    feature_set.iloc[index_var,0] = name\n",
    "    feature_set.iloc[index_var, 1] = data.iloc[0, 2]\n",
    "    feature_set.iloc[index_var, 2:36] = grouped_features.get_group(name).min().iloc[3:]\n",
    "    feature_set.iloc[index_var, 36:70] = grouped_features.get_group(name).max().iloc[3:]\n",
    "    feature_set.iloc[index_var, 70:104] = grouped_features.get_group(name).median().iloc[3:]\n",
    "    \n",
    "    index_var = index_var + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a452d-7e4a-4a7e-9a2f-70a22b8c6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(df):\n",
    "    data = df[:,3]\n",
    "    new_data_ = data.reshape((12, np.around(data.shape[1] * data.shape[0] / 12)))    \n",
    "\n",
    "    training_data = np.array(new_data)\n",
    "    for index, _ in enumerate(np.around(df.shape[0]/12)):\n",
    "        training_data[index,:] = new_data(data[12*i: 12+12*i, ::])\n",
    "\n",
    "    return training_data\n",
    "\n",
    "X_train = get_feature(X)\n",
    "X_test = get_feature(X_test)\n",
    "pd.DataFrame(X_train).to_csv('X_train_stat.csv', index=False)\n",
    "pd.DataFrame(X_test).to_csv('X_test_stat.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12077f3b-daf3-4f71-88f4-57b7f562ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute proportion of missing values\n",
    "tot_rows = feature_set.shape[0]\n",
    "prop_missing_vals = {}\n",
    "missing_values = 0\n",
    "\n",
    "\n",
    "for col_name in feature_set.columns:\n",
    "    missing_vals = np.sum(feature_set.loc[:, col_name].isna())\n",
    "    missing_values = missing_values + missing_vals\n",
    "    prop_missing_vals[col_name] = missing_vals / tot_rows\n",
    "\n",
    "    #if prop_missing_vals[col_name] > 0.8:\n",
    "    #    X.drop(col_name, inplace=True, axis=1)\n",
    "#print(missing_values)\n",
    "def sort_dict(d: dict) -> dict:\n",
    "    return {k: v for k, v in sorted(d.items(), key=lambda item: item[1])}\n",
    "    \n",
    "prop_missing_vals = sort_dict(prop_missing_vals)\n",
    "#print(prop_missing_vals)\n",
    "\n",
    "props = list(prop_missing_vals.values())\n",
    "plt.plot(props)\n",
    "plt.suptitle(\"Proportion of missing values per feature\", size=16)\n",
    "plt.xlabel(\"Feature index (sorted order)\")\n",
    "plt.ylabel(\"Proportion of values missing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
